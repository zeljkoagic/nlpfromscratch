{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from dependency_decoding import chu_liu_edmonds\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent = np.load(\"en.1.npz\")\n",
    "T_proj = sent['projection_tensor']\n",
    "source_langs = sent['source_languages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_correct(pred_heads, gold_heads):\n",
    "    assert len(pred_heads) == len(gold_heads)\n",
    "    # Do not count the root attachment at index 0 as correct\n",
    "    return sum(pred == gold for pred, gold in zip(pred_heads, gold_heads)) - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.704729</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.342184</td>\n",
       "      <td>0.286721</td>\n",
       "      <td>0.626874</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.157235</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.484111</td>\n",
       "      <td>0.374991</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.077324</td>\n",
       "      <td>1.251576</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.384573</td>\n",
       "      <td>-1.437115</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.338265</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.005476</td>\n",
       "      <td>-0.408782</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.193533</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.069444</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.912941</td>\n",
       "      <td>-1.240255</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.220388</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.429097</td>\n",
       "      <td>-0.033233</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.282399</td>\n",
       "      <td>0.182695</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.512245</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.424537</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.502257</td>\n",
       "      <td>-0.541087</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.445114</td>\n",
       "      <td>0.754339</td>\n",
       "      <td>1.101407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.911649</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.167354</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.434388</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.016517</td>\n",
       "      <td>1.103517</td>\n",
       "      <td>0.410408</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.842590</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.246992</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1.555405</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.245161</td>\n",
       "      <td>1.008276</td>\n",
       "      <td>0.561961</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.762564</td>\n",
       "      <td>-0.510424</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.211470</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.554924</td>\n",
       "      <td>0.728259</td>\n",
       "      <td>1.756062</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.988084</td>\n",
       "      <td>-0.291733</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.113105</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1   2         3         4         5   6         7   \\\n",
       "0        NaN       NaN NaN       NaN       NaN       NaN NaN       NaN   \n",
       "1  -2.704729       NaN NaN  1.342184  0.286721  0.626874 NaN  0.157235   \n",
       "2        NaN       NaN NaN       NaN       NaN       NaN NaN       NaN   \n",
       "3  -2.484111  0.374991 NaN       NaN  0.077324  1.251576 NaN  0.384573   \n",
       "4  -2.005476 -0.408782 NaN -0.193533       NaN  1.069444 NaN  0.912941   \n",
       "5  -0.429097 -0.033233 NaN -0.282399  0.182695       NaN NaN  0.512245   \n",
       "6        NaN       NaN NaN       NaN       NaN       NaN NaN       NaN   \n",
       "7  -1.502257 -0.541087 NaN -0.445114  0.754339  1.101407 NaN       NaN   \n",
       "8        NaN -0.434388 NaN  0.016517  1.103517  0.410408 NaN  1.842590   \n",
       "9        NaN       NaN NaN       NaN       NaN       NaN NaN       NaN   \n",
       "10 -1.555405       NaN NaN -0.245161  1.008276  0.561961 NaN  1.762564   \n",
       "11       NaN -0.211470 NaN -0.554924  0.728259  1.756062 NaN  0.988084   \n",
       "\n",
       "          8   9         10  11  \n",
       "0        NaN NaN       NaN NaN  \n",
       "1        NaN NaN       NaN NaN  \n",
       "2        NaN NaN       NaN NaN  \n",
       "3  -1.437115 NaN -0.338265 NaN  \n",
       "4  -1.240255 NaN -0.220388 NaN  \n",
       "5        NaN NaN -0.424537 NaN  \n",
       "6        NaN NaN       NaN NaN  \n",
       "7  -0.911649 NaN -0.167354 NaN  \n",
       "8        NaN NaN  0.246992 NaN  \n",
       "9        NaN NaN       NaN NaN  \n",
       "10 -0.510424 NaN       NaN NaN  \n",
       "11 -0.291733 NaN  0.113105 NaN  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_root_row(T):\n",
    "    first_row = np.zeros([1, T.shape[1], T.shape[2]]) * np.nan\n",
    "    return np.vstack([first_row, T])\n",
    "    \n",
    "T_proj = add_root_row(T_proj)\n",
    "T_proj_de = T_proj[:,:,4]\n",
    "\n",
    "\n",
    "\n",
    "D = pd.DataFrame(T_proj_de)\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.7047291972199998"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nanmin(T_proj_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1, 3, 0, 5, 5, 0, 0, 5, 7, 0, 7, 5] [-1  3  3  4  5  0  7  5  7 10  7  5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_proj_de[2] = np.nanmin(T_proj_de)\n",
    "T_proj_de[6] = np.nanmin(T_proj_de)\n",
    "T_proj_de[9] = np.nanmin(T_proj_de)\n",
    "\n",
    "pred_heads, tree_score = chu_liu_edmonds(T_proj_de)\n",
    "gold_heads = np.concatenate([[-1], sent[\"heads\"]])\n",
    "print(pred_heads, gold_heads)\n",
    "count_correct(pred_heads, gold_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170886\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>tree_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>all_max</th>\n",
       "      <td>70562</td>\n",
       "      <td>106223.742222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_mean</th>\n",
       "      <td>82056</td>\n",
       "      <td>60835.548197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_mean_softmax</th>\n",
       "      <td>82017</td>\n",
       "      <td>10561.636096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_sum</th>\n",
       "      <td>92572</td>\n",
       "      <td>530052.556151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_sum_l1</th>\n",
       "      <td>91915</td>\n",
       "      <td>44996.791306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_sum_softmax</th>\n",
       "      <td>92098</td>\n",
       "      <td>57747.925749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ar</th>\n",
       "      <td>22982</td>\n",
       "      <td>27908.020211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bg</th>\n",
       "      <td>49493</td>\n",
       "      <td>46979.034017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cs</th>\n",
       "      <td>39859</td>\n",
       "      <td>37020.670519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>da</th>\n",
       "      <td>44346</td>\n",
       "      <td>42858.356565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>de</th>\n",
       "      <td>53713</td>\n",
       "      <td>46135.550999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>es</th>\n",
       "      <td>54927</td>\n",
       "      <td>46195.753264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eu</th>\n",
       "      <td>14909</td>\n",
       "      <td>24161.226905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fa</th>\n",
       "      <td>34432</td>\n",
       "      <td>36779.074326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fi</th>\n",
       "      <td>35677</td>\n",
       "      <td>34042.808292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr</th>\n",
       "      <td>54497</td>\n",
       "      <td>46702.526167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>24612</td>\n",
       "      <td>26286.986200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hi</th>\n",
       "      <td>29336</td>\n",
       "      <td>42935.413367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hr</th>\n",
       "      <td>33283</td>\n",
       "      <td>39893.821868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>21737</td>\n",
       "      <td>30919.652654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>45892</td>\n",
       "      <td>42433.511537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no</th>\n",
       "      <td>54953</td>\n",
       "      <td>47029.444199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pl</th>\n",
       "      <td>43105</td>\n",
       "      <td>40207.285096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt</th>\n",
       "      <td>49032</td>\n",
       "      <td>43441.855292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sl</th>\n",
       "      <td>43387</td>\n",
       "      <td>44832.513082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sv</th>\n",
       "      <td>49892</td>\n",
       "      <td>46154.286594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  accuracy     tree_score\n",
       "all_max              70562  106223.742222\n",
       "all_mean             82056   60835.548197\n",
       "all_mean_softmax     82017   10561.636096\n",
       "all_sum              92572  530052.556151\n",
       "all_sum_l1           91915   44996.791306\n",
       "all_sum_softmax      92098   57747.925749\n",
       "ar                   22982   27908.020211\n",
       "bg                   49493   46979.034017\n",
       "cs                   39859   37020.670519\n",
       "da                   44346   42858.356565\n",
       "de                   53713   46135.550999\n",
       "es                   54927   46195.753264\n",
       "eu                   14909   24161.226905\n",
       "fa                   34432   36779.074326\n",
       "fi                   35677   34042.808292\n",
       "fr                   54497   46702.526167\n",
       "he                   24612   26286.986200\n",
       "hi                   29336   42935.413367\n",
       "hr                   33283   39893.821868\n",
       "id                   21737   30919.652654\n",
       "it                   45892   42433.511537\n",
       "no                   54953   47029.444199\n",
       "pl                   43105   40207.285096\n",
       "pt                   49032   43441.855292\n",
       "sl                   43387   44832.513082\n",
       "sv                   49892   46154.286594"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import warnings\n",
    "#warnings.filterwarnings('ignore', r\"\")\n",
    "\n",
    "p = Path(\".\")\n",
    "scores = defaultdict(list)\n",
    "sent_lens = []\n",
    "tree_scores = defaultdict(list)\n",
    "\n",
    "def softmax(sentence_matrix, temperature=1.0):\n",
    "    m_exp = np.exp(sentence_matrix/temperature)\n",
    "    return (m_exp.T / np.nansum(m_exp, axis=1)).T\n",
    "\n",
    "def l1_normalize(M):\n",
    "    return (M.T / np.nansum(M, axis=1)).T\n",
    "\n",
    "def eliminate_all_nan_rows(M_proj):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        by_row = np.nanmax(M_proj, axis=1)\n",
    "        all_nan_rows = np.isnan(by_row)\n",
    "        M_proj[all_nan_rows] = np.nanmin(M_proj)\n",
    "\n",
    "def eval_projection(M_proj, name):\n",
    "    eliminate_all_nan_rows(M_proj)\n",
    "    pred_heads_from_all, tree_score = chu_liu_edmonds(M_proj)\n",
    "    scores[name].append(count_correct(pred_heads_from_all, gold_heads))\n",
    "    tree_scores[name].append(tree_score)\n",
    "\n",
    "def apply_softmax_per_row_per_lang(T_proj):\n",
    "    for lang in range(T_proj.shape[2]):\n",
    "        T_proj_lang = T_proj[:,:,lang]\n",
    "        T_proj[:,:,lang] = softmax(T_proj_lang, temperature=1)\n",
    "    \n",
    "token_count = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for file in p.glob(\"*.npz\"):\n",
    "    sent = np.load(str(file))\n",
    "    T_proj = sent['projection_tensor']\n",
    "    \n",
    "    #print(T_proj.shape[2])\n",
    "    #if T_proj.shape[2] < 15:\n",
    "    #    print(T_proj.shape[2])\n",
    "    #    continue\n",
    "    \n",
    "    source_langs = sent['source_languages']\n",
    "    T_proj = add_root_row(T_proj)\n",
    "    \n",
    "    apply_softmax_per_row_per_lang(T_proj)\n",
    "    \n",
    "    gold_heads = np.concatenate([[-1], sent[\"heads\"]])\n",
    "    for lang in range(T_proj.shape[2]):\n",
    "        lang_name = source_langs[lang]\n",
    "        \n",
    "        T_proj_lang = T_proj[:,:,lang]\n",
    "        T_proj_lang = np.where(np.isnan(T_proj_lang), np.nanmin(T_proj_lang), T_proj_lang)\n",
    "        pred_heads, tree_score = chu_liu_edmonds(T_proj_lang)\n",
    "        \n",
    "        scores[lang_name].append(count_correct(pred_heads, gold_heads))\n",
    "        tree_scores[lang_name].append(tree_score)\n",
    "        \n",
    "        \n",
    "    # Aggregate measures\n",
    "    good_lang_ids = [i for i, lang_name in enumerate(source_langs)\n",
    "                     if lang_name in [\"fr\", \"de\", \"es\", \"sv\", \"no\", \"da\"]]\n",
    "    \n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        eval_projection(np.nansum(T_proj, axis=2), \"all_sum\")\n",
    "        eval_projection(np.nanmean(T_proj, axis=2), \"all_mean\")\n",
    "        eval_projection(np.nanmax(T_proj, axis=2), \"all_max\")\n",
    "\n",
    "        eval_projection(softmax(np.nansum(T_proj, axis=2)), \"all_sum_softmax\")\n",
    "        eval_projection(softmax(np.nanmean(T_proj, axis=2)), \"all_mean_softmax\")\n",
    "\n",
    "        eval_projection(l1_normalize(np.nansum(T_proj, axis=2)), \"all_sum_l1\")\n",
    "    \n",
    "    \n",
    "    sent_lens.append(len(pred_heads) - 1)\n",
    "    token_count += len(pred_heads) - 1\n",
    "\n",
    "aggregated_scores = {k: sum(vals) for k, vals in scores.items()}\n",
    "aggregated_tree_scores = {k: np.nansum(vals) for k, vals in tree_scores.items()}\n",
    "print(token_count)\n",
    "pd.DataFrame({\"tree_score\": pd.Series(aggregated_tree_scores), \n",
    "              \"accuracy\": pd.Series(aggregated_scores)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x10ce77710>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAECCAYAAAAfE3cCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFRlJREFUeJzt3XFsnPd93/E3nYiSFR6ZyDjFS6LamZt8oQFTM7tGVieT\nZNS1q6KJhmLAUKRok6HSjAiaUWTOFjYwsEKOg7nKZgGp0UrL5ABdYURIs7SCMhdaIClCO8GZt1WQ\n87WVLrYCGDYrUhRpVSUlcn/cqWQ0indH3pFH/t4vQNDd8/zuue/9ePe5u9/ze57rmZ6eRpK0ut22\n3AVIkjrPsJekAhj2klQAw16SCmDYS1IBDHtJKsA751sZEWuA54C7gOvArvr/h4Ep4CywJzOnI2IX\nsBu4BuzLzKMdrFuS1IJGn+x/CXhHZn4M+B3gS8B+YDAztwI9wM6IuBPYCzwAPAI8FRG9nStbktSK\neT/ZAwm8MyJ6gAFgAvhoZp6srz8GPEzt0/7pzJwEJiPiPLAFeLEzZUuSWtEo7N8G7gZ+ANwBfALY\nOmv9GLU3gX5gdI7lkqQu0GgY57eA72RmAB8Bvg6smbW+H7gEXAYqs5ZXgJE21ilJWoRGn+yHgcn6\n5ZF6+5ciYltmngB2AMeBM8CTEbEWWAdsprbz9pamp6ene3p6FlO7JJVoQcHZM9+J0CLiXcDXgL8H\n9AL/Efg+cLB+/Rywqz4b5zepzca5DXgyM/+4wX1PDw2NLaTmVadarWBf1NgXM+yLGfbFjGq10v6w\n7zDDvs4n8gz7YoZ9McO+mLHQsPegKkkqgGEvSQUw7CWpAIa9JBXAsJekAhj2klQAw16SCmDYS1IB\nDHtJKoBhL0kFMOwlqQCGvSQVwLCXpAIY9pJUAMNekgpg2EtSAQx7SSqAYS9JBWj0g+NaYSYmJrhw\n4bWWbrNp01309vZ2qCJJ3cCwX2UuXHiNx57+NusHNjbV/sroWzzz+Ce5554PdbgyScupYdhHxG8A\nn65fvR34GeDjwDPAFHAW2JOZ0xGxC9gNXAP2ZebRThSt+a0f2Ejfe96/3GVI6iINx+wz87nMfDAz\nHwReBPYCTwCDmbkV6AF2RsSd9XUPAI8AT0WEYwOS1AWa3kEbET8L/IPMPATcl5kn66uOAQ8B9wOn\nM3MyMy8D54Et7S5YktS6VmbjDAL/rn65Z9byMWAA6AdG51guSVpmTYV9RLwb+HBmnqgvmpq1uh+4\nBFwGKrOWV4CRdhQpSVqcZmfjbAWOz7r+UkRsq4f/jvq6M8CTEbEWWAdsprbz9paq1cp8q4vSrr4Y\nGelr+TYbNvR11d+im2pZbvbFDPticZoN+w8DP5x1/XPAwfoO2HPAkfpsnAPAKWrfGAYzc2K+jQ4N\njS2g5NWnWq20rS+Gh8cXdJtu+Vu0sy9WOvtihn0xY6Fvek2FfWb+7k3XXwW2z9HuEHBoQZVIkjrG\n0yVIUgEMe0kqgGEvSQUw7CWpAIa9JBXAsJekAhj2klQAw16SCmDYS1IB/KWqZXDzTweOjPTNe5oD\nfzZQ0mIZ9suglZ8O9GcDJbWDYb9M/OlASUvJMXtJKoBhL0kFMOwlqQCGvSQVwLCXpAIY9pJUAMNe\nkgrgPHs17eYjf5vh0b9Sd2gY9hHxBeATQC/we8BJ4DAwBZwF9mTmdETsAnYD14B9mXm0U0VrebRy\n5C949K/UTeYN+4jYDvxcZj4QEe8C/jWwHxjMzJMR8SywMyL+AtgL3AfcDnwvIv4sMyc6W76Wmkf+\nSitTozH7h4G/jIhvAX8C/ClwX2aerK8/BjwE3A+czszJzLwMnAe2dKhmSVKLGg3jVIFNwC8Df59a\n4PfMWj8GDAD9wOgcyyVJXaBR2P818HJmXgNeiYirwOzv8P3AJeAyUJm1vAKMNLrzarXSqMmqNDLS\n11L7DRv6mu6rVrfdyvY7ue3ZSn1ezMW+mGFfLE6jsP8e8BjwlYh4H7AeOB4R2zLzBLADOA6cAZ6M\niLXAOmAztZ238xoaGltM7SvWfOeuv1X7Zvuq1W23sv1ObvuGarVS7PPiZvbFDPtixkLf9OYN+8w8\nGhFbI+IMtfH9zwI/Ag5GRC9wDjhSn41zADhVbzfozllJ6h4Np15m5r+ZY/H2OdodAg61oSZJUpt5\nBK0kFcCwl6QCGPaSVADDXpIKYNhLUgEMe0kqgGEvSQUw7CWpAP54SZebun6N119v/gdDWmkrqRyG\nfZe7On6R/c8Ps37gjabaX/zxy9zxgc0drkrSSmPYrwCt/GDIldE3O1yNpJXIMXtJKoBhL0kFMOwl\nqQCGvSQVwB20hWtlaqfTOqWVy7AvXCtTO53WKa1chr2antrptE5p5XLMXpIKYNhLUgGaGsaJiP8J\njNav/hXwFHAYmALOAnsyczoidgG7gWvAvsw82vaKJUktaxj2EbEOIDMfnLXs28BgZp6MiGeBnRHx\nF8Be4D7gduB7EfFnmTnRmdIlSc1q5pP9zwDrI+K/1dv/NnBvZp6srz8GPAxcB05n5iQwGRHngS3A\ni+0vW5LUimbG7N8Gns7MR4BHgT+8af0YMAD0MzPUM3u5JGmZNfPJ/hXgPEBmvhoRF4F/NGt9P3AJ\nuAxUZi2vACPzbbharcy3etUaGelb7hKWzIYNfS3/nUt9XszFvphhXyxOM2H/L4B/COyJiPdRC/EX\nImJbZp4AdgDHgTPAkxGxFlgHbKa28/aWhobGFlP7ijU8PL7cJSyZ4eHxlv7O1Wql2OfFzeyLGfbF\njIW+6TUT9v8JOBwRp4Bp4DPAReBgRPQC54Aj9dk4B4BT1IaHBt05K0ndoWHY13e4fmqOVdvnaHsI\nOLT4siRJ7eRBVZJUAMNekgpg2EtSAQx7SSqAYS9JBTDsJakAhr0kFcCwl6QCGPaSVADDXpIKYNhL\nUgEMe0kqgGEvSQUw7CWpAM2cz15aEhMTE7zyyitN/7jLpk130dvb2+GqpNXBsFfXuHDhNR57+tus\nH9jYsO2V0bd45vFPcs89H1qCyqSVz7BXV1k/sJG+97x/ucuQVh3H7CWpAIa9JBXAsJekAjQ1Zh8R\nG4HvAz8PTAGH6/+fBfZk5nRE7AJ2A9eAfZl5tCMVS5Ja1vCTfUSsAX4feBvoAb4CDGbm1vr1nRFx\nJ7AXeAB4BHgqIpwTJ0ldoplhnKeBZ4E36tfvzcyT9cvHgIeA+4HTmTmZmZeB88CWdhcrSVqYecM+\nIj4NDGXmC/VFPfV/N4wBA0A/MDrHcklSF2g0Zv8ZYDoiHgI+AjwHVGet7wcuAZeByqzlFWCk0Z1X\nq5VGTValkZG+5S5hyWzY0Nf037nVfmll2yvVan98rbAvFmfesM/MbTcuR8R3gUeBpyNiW2aeAHYA\nx4EzwJMRsRZYB2ymtvN2XkNDY4sofeVq9nQAq8Hw8HjTf+dW+6WVba9E1WplVT++VtgXMxb6ptfq\nEbTTwOeAg/UdsOeAI/XZOAeAU9SGhgYzc2JBFUmS2q7psM/MB2dd3T7H+kPAoTbUJElqM8+N0wYT\nExNcuPBa0+1ff735tpLUDoZ9G7RytkaAiz9+mTs+sLnDVUnSDMO+TVo5W+OV0Tc7XE13mLp+raVv\nMX7jkTrHsFfHXB2/yP7nh1k/8EbjxviNR+okw14d5TceqTt41ktJKoBhL0kFMOwlqQCGvSQVwB20\nc/AgKUmrjWE/Bw+SkrTaGPa34JRBSauJY/aSVADDXpIKYNhLUgEMe0kqgGEvSQUw7CWpAIa9JBXA\nsJekAjQ8qCoi3gEcBD4MTAOPAn8LHAamgLPAnsycjohdwG7gGrAvM492qG5JUgua+WT/y8BUZn4c\n+CLwJWA/MJiZW4EeYGdE3AnsBR4AHgGeiojezpQtSWpFw7DPzP8K/Mv61buBEeC+zDxZX3YMeAi4\nHzidmZOZeRk4D2xpe8WSpJY1NWafmdcj4jngGeAPqX2av2EMGAD6gdE5lkuSllnTJ0LLzN+IiPcC\nZ4B1s1b1A5eAy0Bl1vIKtW8Bt1StVuZbvWxGRvqWuwQ1YcOGvq59DrXLan98rbAvFqeZHbS/Bnwg\nM78M/A1wHXgxIrZl5glgB3Cc2pvAkxGxltqbwWZqO29vaWhobJHld8bw8Phyl6AmDA+Pd+1zqB2q\n1cqqfnytsC9mLPRNr5lP9t8E/nNEnADWAI8BPwAO1nfAngOO1GfjHABOURseGszMiQVVJUlqq4Zh\nn5lXgH8+x6rtc7Q9BBxafFmSpHbyoCpJKoBhL0kFMOwlqQCGvSQVwLCXpAIY9pJUAMNekgpg2EtS\nAQx7SSqAYS9JBWj6rJcr3cTEBBcuvNZU29dfb66dJK0UxYT9hQuv8djT32b9wMaGbS/++GXu+MDm\nJahKkpZGMWEPsH5gI33veX/DdldG31yCarQYU9evtfQNbHJyEoA1a9Y0fZtNm+6it9df1tTqUFTY\na/W4On6R/c8Ps37gjabaX/zxy9xeuaOpb3YAV0bf4pnHP8k993xoMWVKXcOw14rV7Dc1qH1ba6W9\ntNo4G0eSCmDYS1IBDHtJKoBhL0kFMOwlqQDzzsaJiDXA14C7gLXAPuBl4DAwBZwF9mTmdETsAnYD\n14B9mXm0g3VLklrQ6JP9p4ChzNwK/CLwVWA/MFhf1gPsjIg7gb3AA8AjwFMR4dEoktQlGs2z/wZw\npH75NmASuDczT9aXHQMeBq4DpzNzEpiMiPPAFuDF9pcsSWrVvGGfmW8DRESFWvB/EfjdWU3GgAGg\nHxidY7kkqQs0PII2IjYB3wS+mpl/FBH/ftbqfuAScBmozFpeAUYabbtarTRq0jYjI31Ldl9aHTZs\n6FvS5+hclvv+u4l9sTiNdtC+F3gB+Gxmfre++KWI2JaZJ4AdwHHgDPBkRKwF1gGbqe28ndfQ0Nhi\nam/J8PD4kt2XVofh4fElfY7erFqtLOv9dxP7YsZC3/QafbIfpDYc80REPFFf9hhwoL4D9hxwpD4b\n5wBwitrY/mBmTiyoIklS2zUas3+MWrjfbPscbQ8Bh9pTliSpnTyoSpIKYNhLUgEMe0kqgGEvSQUw\n7CWpAIa9JBXAsJekAhj2klQAw16SCmDYS1IBDHtJKoBhL0kFMOwlqQCGvSQVwLCXpAIY9pJUAMNe\nkgpg2EtSAQx7SSqAYS9JBZj3B8dviIiPAl/OzAcj4qeBw8AUcBbYk5nTEbEL2A1cA/Zl5tEO1SxJ\nalHDT/YR8XngILC2vugrwGBmbgV6gJ0RcSewF3gAeAR4KiJ6O1OyJKlVzQzjnAd+hVqwA9ybmSfr\nl48BDwH3A6czczIzL9dvs6XdxUqSFqbhME5mfjMi7p61qGfW5TFgAOgHRudYLq1IU9ev8frrrzXd\nftOmu+jt9cusuldTY/Y3mZp1uR+4BFwGKrOWV4CRRhuqViuNmrTNyEjfkt2XVr6r4xfZ//ww6wfe\naNh2fOQN9j36cT74wQ82vf277767qTeHpXyNdDv7YnEWEvYvRcS2zDwB7ACOA2eAJyNiLbAO2Ext\n5+28hobGFnD3CzM8PL5k96XVYf3ARvre8/6G7a6MvskTf/DnrB/4YVPbvTL6Fs88/knuuedD87ar\nVitL+hrpZvbFjIW+6bUS9tP1/z8HHKzvgD0HHKnPxjkAnKK2H2AwMycWVJG0AjX7xiAtl6bCPjN/\nRG2mDZn5KrB9jjaHgENtrE2S1CYeVCVJBTDsJakAhr0kFcCwl6QCGPaSVADDXpIKYNhLUgEMe0kq\nwEJOlyBpEZo9ydrISN/fnebDE61psQx7aYm1cpI1aP5cOtJ8DHtpGXguHS01x+wlqQB+spe6nD+k\nonYw7KUu18oYv+P7upUVG/bj4+P8q99+mne9+71NtR9981UYuLfDVUmd4Ri/FmvFhv309BRX19wJ\nt0dT7a+9460OVyRJ3csdtJJUAMNekgpg2EtSAdo6Zh8RtwG/B2wB/hb4zcz8YTvvQ9KttTpNE5yq\nWYp276D9p0BvZj4QER8F9teXSVoCrZ6KYXzkDR7/1Xv5qZ+6q6n2vjGsXO0O+48B3wHIzP8RET/b\n5u1LaqCVaZpXRt9k//P/2zn8BWh32PcDl2ddvx4Rt2XmVJvvR1KbNPvm0OoQ0eTkJABr1qxp+jZ+\nc+icdof9ZaAy63pHg/7a6P9lqmeiqbYTY29y9XqlcUPgb8aGgZ6m6+hke2uxlm6pZeSNV9l38Aes\n69vQVPvRN/+Kte96d9Ptr44P88VdvzDnkNLs0z13o5XwbafdYX8a+ATwjYj4x8D/madtT7XaXPjO\npVqt8N1vPbvg20tSSdod9n8M/EJEnK5f/0ybty9JWoCe6enp5a5BktRhHlQlSQUw7CWpAIa9JBXA\nsJekAiz5+exLP39ORKwBvgbcBawF9gEvA4eBKeAssCczi9lzHhEbge8DP0+tDw5TYF9ExBeoTV3u\npfYaOUmBfVF/jTxH7TVyHdhV//8wBfVF/ZQzX87MByPip5nj8UfELmA3cA3Yl5lHb7W95fhk/3fn\nzwH+LbXz55TkU8BQZm4FfhH4KrU+GKwv6wF2LmN9S6r+wv594G1qj/0rFNgXEbEd+Ln662IbsIly\nnxe/BLwjMz8G/A7wJQrri4j4PHCQ2gdCmON1ERF3AnuBB4BHgKci4paHHy9H2P/E+XOA0s6f8w3g\nifrl24BJ4N7MPFlfdgx4aDkKWyZPA88CN07OUmpfPAz8ZUR8C/gT4E+B+wrtiwTeGRE9wAAwQXl9\ncR74FWYOb57rdXE/cDozJzPzcv02W261weUI+znPn7MMdSyLzHw7M8cjokIt+L/IT/4dxqk9wVe9\niPg0tW85L9QX9fCTx+4X0xdAFbgP+GfAo8B/ody+eBu4G/gBtW99ByisLzLzm9SGZm6Y/fjHqD3+\nfmB0juVzWo6QXdLz53SjiNgE/Hfg65n5R9TG4W6oAJeWpbCl9xlqR1x/F/gItXHa6qz1JfXFXwMv\nZOa1zHwFuMpPvnBL6ovfAr6TmUHtefF1YPbZ1ErqixtmZ0Q/tcd/c5ZWgJFbbWA5wv40tTE5mjh/\nzqoTEe8FXgA+n5mH64tfioht9cs7qO2YW/Uyc1tmbs/MB4H/Bfw68J0S+wL4HrV9OETE+4D1wPFC\n+2KYmW//I9QmkhT5Gpllrsd/BvgnEbE2IgaAzdR23s5pyWfj4PlzBql9YnsiIm6M3T8GHKjvXDkH\nHFmu4pbZNPA54GBpfZGZRyNia0ScofYh7LPAjyiwL4D/AHwtIk5Sm5n0BWqztUrsixszjv6/10V9\nNs4B4BS158xgZt7yNMCeG0eSClDMjlFJKplhL0kFMOwlqQCGvSQVwLCXpAIY9pJUAMNekgpg2EtS\nAf4fJo6wlKO1k8cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ce77828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "D = pd.DataFrame({\"accuracy\": np.array(scores[\"all_sum\"])  / np.array(sent_lens), \n",
    "                  \"tree_score\": np.array(tree_scores[\"all_sum\"]),\n",
    "                  \"norm_tree_score\": np.array(tree_scores[\"all_sum\"]) / np.array(sent_lens),\n",
    "                  \"sent_lens\": np.array(sent_lens)}\n",
    "                )\n",
    "\n",
    "#for i in range(11):\n",
    "#    print(D[D.accuracy > i / 10].sent_lens.mean())\n",
    "\n",
    "D.sent_lens.hist(bins=25)\n",
    "#D[D.sent_lens > 35].accuracy.mean()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  3,  0,  5,  3,  3,  3,  3, 10,  3, 12, 10, 10, 10,  3, 17, 15,\n",
       "       17,  3, 21, 15, 23,  3, 26, 27, 27, 23, 27, 30, 27, 33, 33, 30, 37,\n",
       "       37, 37, 27,  3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent['heads']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Score matrix must be square",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d8ed125610ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mM_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM_norm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mchu_liu_edmonds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mdependency_decoding.pyx\u001b[0m in \u001b[0;36mdependency_decoding.chu_liu_edmonds (dependency_decoding.cpp:1552)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Score matrix must be square"
     ]
    }
   ],
   "source": [
    "x = np.zeros(M_norm.shape[1]) * np.nan\n",
    "M_norm = np.vstack([x, M_norm])\n",
    "chu_liu_edmonds(M_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
